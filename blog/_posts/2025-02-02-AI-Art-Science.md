---
title: "On AI, Art, and Science"
mathjax: false
layout: post
---

I have no formal art education, but I’ve made art—most of it pretty bad, with some small flashes of beauty. How much good art I’ve made in my life depends on what “art” is. I’ve never had much of a felicity for visual art, but, some of the music I’ve made and composed certainly counts. What about meals I’ve cooked? Hack-y poems that nonetheless feel genuine—candid and personal—products of myself from a time and place? What about a conference seminar, or a lecture series on Physical Biochemistry?

Artificial Intelligence (AI) and Machine Leaning (ML) have very publicly shaken up the Art World and — for lack of a better catch-all term — the world of the Written Word: Hollywood unions fought a long, public battle to (amongst many other things) [keep AI/ML out of writers rooms](https://time.com/6277158/writers-strike-ai-wga-screenwriting/); artists and graphic designers on social media endlessly lament the [theft of their work](https://www.theverge.com/2022/11/15/23449036/deviantart-ai-art-dreamup-training-data-controversy) by companies who trained their AI/ML models on it without permission, and whose models [generate near-copies of their work if prompted specifically enough](https://photutorial.com/stable-diffusion-watermarks-investigation/); writers have found that AI/ML models can mimic their voice and style in clunky but uncanny and foreboding ways. Though the models remain relatively crude and largely unprofitable, for now, there is nevertheless a looming question of whether human dominance in these fields will survive the century, or even the decade.

A lesser known but perhaps more insidious infiltration of AI/ML is happening in the world of science. Researchers have been caught [using Large Language Models (LLMs) to fill in parts of their scientific papers](https://arxiv.org/pdf/2404.01268) and AI/ML methods are being used to [fake scientific figures in ways that are hard to spot using standard fraud-detection techniques](https://scienceintegritydigest.com/). Despite getting more positive press, most insidious of all is the rise of AI/ML algorithms as earnest scientific modeling tools. Peruse the pages of scientific journals these days and you'll be hard pressed to find an issue that doesn’t include a paper using an AI/ML algorithm as a sort of ersatz scientific model. The popular science press is flooded with articles trumpeting the supposed success of AI/ML in fields like medicine, education, and nuclear fusion. These models are now so common that researchers and the broader scientifically-engaged public no longer bat an eye.

I’d argue that we’re watching, across the board, is not just a technological hype cycle but a wholesale abandonment of principle: a deep philosophical battle that we’re all conscripted in. Artists, writers, scientists, the broader public—we're all due for a wake up call. We’re in the early stages of this fight, but the machines and their allies are winning.

<!--more-->

---

While reading a recent [New Yorker piece](https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art) about AI/ML and Art by the science fiction writer Ted Chiang I was pleasantly surprised by his definition:

> ***Art is something that results from making a lot of choices.***

I like this definition of art. It has the benefit of being both easy to understand and no less effective in generalization. 

Though this definition works nicely as a heuristic, it doesn’t map cleanly on to my experience of *making art*. What art I’ve made that’s felt even the slightest bit personal and transcendent was made in a flow state: not quite devoid of choice but unconcerned with it; more akin to surrender than willful effort. But I guess there’s nothing inherently active *or* passive about “making choices.” Maybe, in order to make good choices, it can be as useful to be willful and determined as it is to get out of your own way.

The artist and critic Taylor Morrison [recently gave this advice](https://www.instagram.com/reel/C-GzG1JMYBJ/?igsh=a2RtYjR6MWU1OTc5) to an artist who submitted their work for critique, a self-described beginner and autodidact whose drawings were undoubtedly impressive but stilted: 

> It’s time to be brave. You can draw. Now, *what* are you going to draw? You're bursting at the seams here. It's time make something that isn’t practice. It's time to make something that isn’t learning. ***Make something you can’t understand, and that you don’t wish to.***

Now *this* I could identify with. It sits at the heart of the question above: how can an artist be both a conduit for *and* an agent of choice-making? You endeavor to *just do*—with your entire being *just make*—and in the doing, everything you are will leave the imprint of choice (of taste and proclivity) that amounts to something familiar because you made it but alien because no one, including yourself, has seen it before.

*This picture of what art is, and how good art is made, may seem paradoxical. Art is comprised of choice-making, but is best made when those choices come from a place outside our comprehension? The paradox here is just an illusion: on careful inspection you’ll find that basically everything in your subjective experience is like this. What you do, what you think, who you are in each moment—everything flows from a place outside your comprehension. What we call “taste” is just an accumulation of influence and preference that none of us can really claim responsibility for. Why don’t I like death metal? I can conjure up reasons if I’m asked to, but the honest truth is I just don’t, and I never have. With this in mind, these definitions of art and artistic practice aren’t really contradictory, but illustrative of a deep underlying truth about conscious experience, and how it finds its truest lived expression.*

In his piece in the New Yorker, Chiang goes on to lay out how paradoxically, under his definition, AI/ML companies advertise their products: as both a boon to creatives, and an instrument for the elimination of artistic tedium (i.e. choices):

> If an A.I. generates a ten-thousand-word story based on your prompt, it has to fill in for all of the choices that you are not making.… Real paintings bear the mark of an enormous number of decisions. By comparison, a person using a text-to-image program like DALL-E enters a prompt such as “A knight in a suit of armor fights a fire-breathing dragon,” and lets the program do the rest…. Most of the choices in the resulting image have to be borrowed from similar paintings found online; the image might be exquisitely rendered, but the person entering the prompt can’t claim credit for that.

 It’s fashionable to say that AI/ML products are artistically hollow because they represent only a prompt-guided probabilistic average of all the art they’ve ingested. But it’s not this property that drains the outputs of artistic value. A lot of human art is some kind of weighted combination of influences. To my mind, it’s not even that the AI/ML tools aren’t making any choices. As Chiang says, their purpose is to fill in for all the choices you’re not making.
 
 Rather, their artistic output is hollow because *they have no taste*. Or, what taste they’ve been trained to have suggests only an aesthetic preference for the innocuous (for obvious legal and economic reasons). They have no aesthetic principles. There is nothing they’re aspiring to be, or not to be, because they’re aspiring to be anything. To make anything of true artistic value with these tools, one would have to supply the algorithm with prompts of such specificity, such care, such discrimination and understanding of the algorithm's intricacies and limitations, that the prompt in itself becomes the artistic product, not the models' output.
 
 To be sure, some artists are making work of this kind with AI/ML tools. But this style of engagement with the product is not what's advertised. These companies are selling microwaves. Sure, it's possible to create food of real skill and personal expression using a microwave, but the marketing materials promise gorgeous, succulent, satisfying meals for the whole family at the touch of a button.

Taking stock of what these tools can do, I feel a similar sense of awe and bewilderment as when I listen to [a starling](https://www.instagram.com/reel/DEPn-cZIwCH/?utm_source=ig_web_copy_link&igsh=MzRlODBiNWFlZA==) that's been trained to mimic its owners voice or the bleeps of R2D2. It's wondrous, impressive, and strange, but I'm as impressed by the trainer as I am with the bird. I can only dimly imagine the level of effort, patience, and understanding it must have required. The beauty isn't produced by the bird, or the owner, but in their collaboration.

A lot of artistic production can feel this way. In music, I've found that in moments of lucid free expression, the instrument feels more like a collaborator than a tool. Learn the intricacies of a piano or a guitar or a drum set well enough and its idiosyncrasies come pouring through, sometimes without your direct intention or forethought. 

The thing that feels cheap and disingenuous about the current crop of AI/ML tools for art and language is the sense that their designers want us to view them as artists or writers in their own right, rather than tools or collaborators of this kind. It's right there in the name: the *machine* has *learned* to create, all on its own. I much prefer the [framing of Jaron Lanier](https://www.newyorker.com/science/annals-of-artificial-intelligence/there-is-no-ai), a tech pioneer and critic, when it comes to this point:

> the most accurate way to understand what we are building today is as an innovative form of social collaboration.

---

The Philosopher of Science Karl Popper said (though the exact quotation is probably not his own), “a theory that explains everything explains nothing.” A variation is likely true of art: “an artist that makes anything makes nothing.”

Science and art are often dichotomized—e.g., certain things are said to be “more of an art than a science”—though I’ve become more uncomfortable with this distinction the more time I spend as a working scientist. To make scientific progress feels, subjectively, a lot like making art. The same experience described above—that elusive mingling of will and surrender, the moderating influence of taste—can describe the feeling of discovering or inventing new science as much as discovering or inventing new means of personal expression.

In their *means*, science and art may feel similar to the practitioner, but maybe it’s their *ends* that make them natural opposites. The *end* of art is capturing the subjective, whereas science aims to capture the objective. 

In the interest of irony, I asked chatGPT the following:

> If someone defines art as “something that results from making a lot of choices,” and science is dichotomized as the opposite of art, what would be the corresponding definition of science in this view?

And here’s what it had to say:

> If art is defined as "something that results from making a lot of choices," and science is conceptualized as the opposite of art in this framework, a corresponding definition of science might emphasize minimizing choices. 
> 
> In this view, science could be defined as "**something that results from reducing choices to those dictated by evidence, logic, and necessity.**" While art thrives on subjectivity, interpretation, and the personal preferences of the creator, science seeks objectivity, consistency, and predictability, focusing on the elimination of ambiguity by adhering to well-defined principles and empirical data. The goal of science, therefore, is not to multiply choices but to narrow them down to the singular paths that best explain or describe natural phenomena.

Characteristically for chatGPT, this response is uncanny and impressive (coming from an array of floating point numbers and a bunch of linear algebra, this is fine work). But it’s also characteristically banal, long-winded and pedantic.

For instance, why not just stop at the definition of Science as “something that results from reducing choice.” This would make for the cleanest contrapositive to our proposed definition of art. But most of the definitions of science the AI was trained on probably also made reference to its relationship to “evidence”, “logic”, and “necessity,” so the AI can’t help but to include them. It has no taste for clean symmetry.

Aside from the aesthetic considerations, is this even a good definition of science? Certainly, when contrasted with pseudo-science or non-science, the reduction of choice seems to capture something. Scientific laws, in some sense, define the boundaries of the possible—the "choices" nature is (and we, as a part of nature, are) allowed to make. Faster than light communication or perpetual motion may be possible with astral projection or Qi energy, but, then again, in the pseudo- or non-scientific worlds, anything is possible. And, of course we need to rely on evidence, logic, and necessity to provide for our bounds.

But, this definition also seems to be missing something. New scientific theories can also allow for *more*, previously unknown, choices: one can’t make a modern computer chip without first understanding the semiconductor and the transistor; one can’t make a either of those without first understanding electricity. Scientific advancement can be as much about the *expanding* the domain of choice as it is about *restricting* the domain of the possible.

Moreover, the response from chatGPT demonstrates exactly the point Chiang closes with, in his piece:

>Some have claimed that large language models are not laundering the texts they’re trained on but, rather, learning from them, in the same way that human writers learn from the books they’ve read. But a large language model is not a writer; it’s not even a user of language. Language is, by definition, a system of communication, and it requires an intention to communicate…. What makes the words “I’m happy to see you” a linguistic utterance is not that the sequence of text tokens that it is made up of are well formed; what makes it a linguistic utterance is the intention to communicate something…. [The LLM is] a fundamentally dehumanizing technology because it treats us as less than what we are: creators and apprehenders of meaning. It reduces the amount of intention in the world.

ChatGPT is a text prediction machine, masquerading as an interlocutor. In real conversation, an answer to question isn’t a puzzle to solve, but an invitation to conversation. Sometimes the right answer to a question is another, better question. 

A better question might have been "what's the opposite of making choices?". Maybe “taking direction”?

Is science “the result of taking direction”? A good scientist takes direction from the way the universe behaves. Revisions of scientific theories happen when we take direction from Mother Nature: when she doesn’t conform to our models of Her, we update the models. This definition is probably better (less forced, more generalizable) than the one about reducing choice, but to have come up with it would have required that the AI understand the *intention* of the question, and respond with intention -- not just play a word game, but play with ideas.

In any case, this may not have been a fair test for our poor AI, because, as we’ve already discussed, science and art aren’t really opposites. There’s an art to science, and a science to art. But, then again, if it's really intelligent, shouldn’t it know this?

--- 

Like the practice of art, scientific practice has aesthetics — choices we make about how to conduct scientific inquiry —  principles that are not dictated by evidence, logic, or necessity *per se* but are no less important. A simpler model is preferable to complex one, even if both take direction from Mother Nature: the Ptolemaic model of the solar system with its orbits of nested epicyles could have been continually adapted, to arbitrary accuracy and precision, but the Galilean or Copernican model was better because *it didn’t need to be continuously adapted*. We prefer the latter to the former, in a very real sense, as a matter of aesthetic preference—as a matter of taste—but this preference serves us well.

So, what about a contrapositive to our definition of good art praxis? That in art you should seek to “make something you can’t understand, and don’t wish to.” When asked to produce the scientific mirror-version, chatGPT produced the following (along with its usual filler):

> ***Seek to understand what you don’t know, and commit to wanting to know.***

This is very good, a *modus operandi* I don’t think any scientist would quibble with.

How ironic, then, that the infiltration of AI/ML models into scientific modeling and practice works to the very *antithesis* of this principle. The adoption of artificial neural networks in scientific modeling represents the abandonment of this principle and commitment. 

---
### Science by Oracle

The Universal Approximation Theorems for Artificial Neural Networks are a family of mathematical proofs, which guarantee the following: given any function (of a certain quite general kind) there exists an artificial neural network of some size that can approximate it arbitrarily well. A neural network is, itself, just a function: an extraordinarily complicated non-linear combination of sums and transformations of the input data, with the parameters or "weights" of these combinations tuned during training. When we say “function” here, we essentially mean any relationship between variables or quantities—the kinds of relationships of which our scientific models of the universe are comprised. The functions a neural network approximates can be extremely complicated, and yet, with enough parameters, a neural network can (at least, in principle) represent these relationships to arbitrary precision. As mathematical models, artificial neural networks are as close as we’ve come to “theories of everything.”

At first blush, this seems extraordinary. But there's a catch. The Universal Approximation Theorems are *existence* theorems: they only prove that such a neural network *exists*, they don't provide a means by which to *create* one. In practice, this is where the rubber meets the road. The bewildering pace of growth in AI model size and the insatiable hunger for training data at the marquee AI firms is a testament to just how crucial this caveat is. Sure, there exists a neural network that can approximate natural language use across the entirety of the extant internet, but it costs billions (perhaps trillions) of dollars in data and compute to actually make one.

Assuming this whole thing works, though, one might be tempted to ask: "are we ... done?" Can we kick back while data centers across the globe collapse our collective intellectual and scientific endeavors to the construction and maintenance of these Universal Approximators?

Consider the following thought experiment: 

> In an ancient city in Greece, there was an Oracle. The people would come to her with devotional offerings, and vast reams of catalogued observations concerning the weather, the stars, the properties of certain minerals, the features of various plants, the behaviors of the animals in the region, topographical maps, the distances to neighboring cities, the movements of enemy troops, etc. When she was satisfied with the offerings, the people were allowed to ask her questions — what minerals to mix with what others to produce a substance with a desired quality; what orientations and distances at which to place their catapults to inflict the maximum damage to enemy encampments; what areas in which to plant crops so as to maximize yield. 
> 
> The Oracle’s predictions never seemed to err, so long as she was satisfied with the offerings and had been provided with sufficient observations (they had learned this latter requirement the hard way). She seemed, for all the people knew, to be in complete command of the laws of the Gods. They were totally reliant on her, but she served them well: their society had only become more healthy and prosperous as they served and queried her. 
> 
> Out of what was believed to be religious devotion, the one thing the Oracle would never do was refuse to answer the question. The questions could be absurd: what rock would make the most effective threads for clothing? (“calcite”) What regional dialect of their language would be best for communicating with insects? (“Achean”) But her answers were surely pristine in some sense they could not but trust and accede to.
> 
> More mysteriously, they found she appeared to be at pains to render explanations. She could only tell them *what* she knows, not *how* she knows it. She had operated in this way for so long, they no longer even thought to ask her how she knew what she knew. Her prescriptions and predictions themselves were evidence enough that, somehow, she knew.
> 
> The society that had sprung up in this ancient city was advanced but strange: intricate networks of buildings surrounding her temple housed countless scribes whose only concerns were cataloguing everything one could think to catalogue: some scribes dedicated years of their life to counting and tabulating their heartbeats, others watched their colleagues sleep and recorded their respiratory rate and sleep disturbances. Their water arrived from their mountain reservoir in sophisticated carts of stacked cubes made of sheep skin; the oracle had revealed that a cube was the most efficient shape for transport. They had never thought to ask about more efficient ways to transport water down the mountain.

Can we say that this ancient society is doing science? Though they're doing it through a proxy, they're still taking direction from the laws of Nature, are they not? Their choices are limited with phenomenal efficiency by her inerrant responses. Their model of the universe has but one simple component, and it is her.

What they fail to do is endeavor to *understand*. Understanding doesn’t consist in simply finding the answers to questions, but in refining what questions to ask. This is, fundamentally, how science progresses. We no longer have to ask questions about phlogiston, because we understand that it doesn’t exist. We could ask as many questions as we like about the substance ghosts are made of, but we’d be wasting our time.

This is the reason why “a theory of everything is a theory of nothing.” A model that captures everything captures nothing in particular. It doesn’t help to explain because an explanation requires reduction, not just in the answers it gives but in what questions it allows for in the first place. “That’s just the way it is,” is a good explanation for everything, but a terrible explanation for anything in particular.

Artificial neural networks aren’t really all that different from the Oracle. They can require vast resources, have inexhaustible data requirements, and amount to a model that is so convoluted and inscrutable it’s unreasonable to even ask how it "knows" what it "knows". Seemingly without irony, major AI companies have adopted the technique of teasing apart the "concepts" stored in their Large Language Models' gargantuan collection of weights using—I kid you not—[more deep neural networks](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).

---

Of course, there's a reason people are doing this: *it works*. There are things you can do with artificial neural networks that seemed impossible only a few decades ago. 

[Levinthal's paradox](https://web.archive.org/web/20110523080407/http://www-miller.ch.cam.ac.uk/levinthal/levinthal.html) refers to a seemingly insurmountable problem in the science of protein folding. Proteins are the biological machines that do the work of... well, everything that needs to get done in a living thing. They contract and expand to power muscles, they digest our food, they block and shuttle ions across our cell membranes to pass along signals in our brains. Proteins are composed of long chains of amino acids: molecules of different sizes, shapes, and properties that click together like little modules to form molecular machines with a specific shape and function. Click any significant number of these animo acids together and the number of different configurations the whole protein can be found in becomes incomprehensibly vast: assuming each amino acid can adopt one of 3 different poses (an extremely conservative assumption) a 200 amino acid protein (small, but reasonable as an illustrative case) can adopt 3^200 different conformations. There are 4 quadrillion possible configurations of this small-ish protein for every atom in the observable universe.

*Protein folding is an extremely hard problem.* 

The crux of Levinthal's paradox is not even how quixotic it might be for humans to ever predict the structure that a given protein folds in to, but how the *protein's themselves* could ever actually fold in to their specific shapes, so consistently, given the number of different possible configurations. This is a seemingly miraculous trick that mother nature pulls, and each of our lives depend on it happening quadrillions of times a second.

AlphaFold is the brand name for a series of protein structure prediction algorithms by Google's DeepMind that have revolutionized Structural Biology. If you read the popular science press you may have heard that DeepMind has "solved the protein folding problem" (reader, I'll just have to ask you trust me: they have not). The [Nobel Prize recently awarded](https://www.nobelprize.org/prizes/chemistry/2024/summary/) to these advances was controversial, but I don't think any scientist could convincingly minimize the gravity of the accomplishment (at the very least as an *engineering* achievement). Given any sequence of amino acids, AlphaFold can provide a reasonably good guess at how the corresponding protein will fold.

This is the kind of thing neural networks were made for: whatever function we can imagine that would take in a sequence of amino acids and output coordinates for all the atoms in the corresponding protein is bound to be extremely complicated; so complicated that even imagining a symbolic mathematical model for it exhausts the mind. But that doesn't matter, a neural network can approximate it. This problem also has the benefit of being very well-constrained, and (owing to the thankless work of thousands of graduate students, post docs, and professional scientists across decades) we have a large database called the [Protein Data Bank (PDB)](https://www.rcsb.org/) of extremely high-quality, free, open access protein structure data to train on. Natural language text prediction is a similarly well-constrained problem (we're unlikely to invent a new form language with entirely new syntax and semiotics tomorrow), and the internet provided what at first appeared to be more data than we could have ever wished for. 

This is not to say that either of these problem are *easy*, only that they're well-suited for a neural network. The kinds of problems that don't (yet) appear to be well-suited to neural networks are un-constrained and/or data-limited -- the kinds of things children and young adults excel at learning: things like object manipulation, task generalization, adaptation to out-of-sample events, and spatial or physical understanding.

But, even if we were to eventually build neural networks that can perform as well as humans at *all* of these problems, the question (at least from a scientific standpoint) is "what have we learned?" What do we *understand* now, that we didn't before? What new questions can we ask, that we couldn't even think to ask before?

When it comes to protein folding, I'm not sure I can say that AlphaFold gives us a new or better *understanding* of protein structure, or the protein folding problem. I'm not sure what the model *teaches* us. I'm not sure what we've *learned* other than: "a computer can predict protein structure very well". Again, this is an astonishing achievement, but among the list of the most important or beautiful things we've learned about the Laws of Nature, this insight is pretty uninspiring.

We can say much the same thing about Large Language Models: it is astounding that computers can be wrangled in to producing such an uncanny and seemingly expert command of language and the information it conveys. But can we say that these models have taught us anything about language and meaning? Can we even imagine *how* they would teach us anything about language and meaning? Perhaps someone can. But given that even state of the art models still seem to struggle with counting how many "r"s are in the word "strawberry", I have my doubts—and I think those who make excuses or play defense for these models are doing more wishful motivated reasoning than rigorous first-principles forecasting.

---

So, how do we distinguish between what artificial neural networks are doing and “true” scientific modeling? What does it really mean, on a practical level, for these models to “teach” us something—to help us *understand* versus simply predict?

Ultimately, as with language, what it comes down to is intention. 

Science isn’t just a method of inquiry, it’s a collective enterprise, a means of communication. In his *New Yorker* piece, Chiang highlights a lack of intention as the characteristic that cheapens Large Language Models' value in the production of language, and in much the same way, artificial neural networks are rendered vacuous as scientific models by their lack of intention. As we've just reviewed, this is by no means to suggest that artificial neural network models are limited in *predictive* power, just in their *expressive* power—their *communicative* value to a *community* with an *intention to understand*.

In language and artistic expression, intention is the desire for understanding. The main thing—sometimes the *only* thing—that separates satire from its genuine counterpart is intention. The best satire is indistinguishable from the thing it satirizes. It is only fully appreciated with the addition of knowledge about its intention. That intention doesn't exist *inside* the text. It exists in the relationship between the artist or the writer and their audience. It's made possible only by the intention to understand and be understood.

In his 2005 book [*On Bullsh-t*](https://en.wikipedia.org/wiki/On_Bullshit), the philosopher Harry Frankfurt draws the following distinction between "bullsh-t" and outright lies: the liar *knows* the truth and *intends* to deceive; the "bullsh-tter" is unconcerned with the truth. The bullsh-tter may know the truth or he may not (he may in fact *say* the truth unintentionally in the course of bullsh-tting) ultimately it doesn't matter—the bullsh-tter is only concerned with getting the message out.

Anyone who has used a Large Language Model is (or—god help us—*should be*) well aware of the ease with which they produce complete fabrications (the term of art in the AI world is "hallucinations"). Techniques like retrieval-augmented generation (RAG) have reined in this problem to some degree, but it's worth dwelling on why it exists in the first place. Why is it that models tuned so painstakingly and exhaustively for predictive accuracy can so easily generate utter drivel, nonsense, and lies?

*It's because they're bullsh-tting.*

Large Language Models don't *intend* to "hallucinate"; they have no *intention* to deceive. They are simply unconcerned with the truth. They are only concerned with outputting the highest probability token to follow those that preceded it. Sometimes that token contributes to a true statement, sometimes it doesn't. The model has no desire, nor any *ability*, to distinguish between the two because truth and lies are composed of the same tokens (sometimes in the same order). It's our desire to *understand* that allows for the distinction. Fine-tuning (devoting additional, high-priority training time to a particular subset of data) and Reinforcement Learning with Human Feedback (querying humans with the model's output and rewarding or penalizing the model based on their feedback) are the field's fledgling attempts at solutions to this problem. But this is teaching the starling to mimic speech. No matter how good it gets, it won't—it can't—develop a better understanding of what the words really *mean* outside the test, the message that tokens in a particular order convey.

*The question remains: does this really matter? If our computers produce a near-perfect simulacrum of understanding, and the gap between the simulacrum and its referent becomes smaller and smaller over time, how important can that gap possibly be? The crucial thing to consider is how good we are at discerning that gap. You may not care that the computer doesn't really understand what it's doing when it creates a detailed plan for your next vacation—if it messes up in small ways, you can easily notice, and adapt. But, how much do you care when it's creating a detailed plan for your child's chemotherapy? If it messes up (in small or big ways) would you notice? How much does it matter to you that, fundamentally, under the hood, the model cannot distinguish between these two tasks? I'd argue you should care quite a bit, and I'd argue you should care even more the worse you are at discerning the gap between understanding and its counterfeit.*

Artificial neural networks make for poor scientific models for more or less exactly the same reason: they're not *incapable* of helping us understand the laws of nature more deeply, they're simply *unconcerned* with it. If they are capable of providing deeper insight, it will be our efforts to extract something coherent and digestible out of the model’s inscrutable parameters that generates these insights, not anything intrinsic to the model itself. Deep neural networks themselves provide no insight. They just predict. They're fabulously good at this, but it's all they do. When a deep neural network spits out predictions that are fatuous or impossible, we don't fault the model—we *can't* fault the model—it's doing what it's supposed to do. It's predicting. But science is about so much more than prediction.

The thing that separates a "true" scientific model from the kind of model represented by a deep neural network is the *intention* to understand the universe more fully, and to do so in such a way that the insights the model provides can be *communicated* effectively. When artificial neural networks fail, we only know that they've failed when we understand the right and wrong answers ahead of time; barring this, we have nothing to do but test and evaluate their predictions within a rubric *we understand*. There's a name for this human-comprehensible-testing-and-validation step: it's called *Science*.

Humans have tried, and will continue to try, to squeeze insight out of AI/ML models in more elaborate and clever ways, but this kind of effort is not required of scientific models like Thermodynamics, or Newton's Laws, or General Relativity, or the Theory of Natural Selection because the models were *constructed with that intention to begin with*. Modeling in this way may be more difficult, it may require time and patience and genius and serendipity and human collaboration, but so does almost everything else in our collective endeavor that’s good and beautiful.

The alternative is Science by Oracle. The alternative is being *subjected* to bewildering scientific advances made by models whose epistemological structure we don't understand and whose consequences we can't foresee. The alternative is medicine designed by a model whose understanding of the difference between drug and poison we [can’t reliably parse](https://pmc.ncbi.nlm.nih.gov/articles/PMC9872092/). The alternative is [nuclear fusion reactors](https://www.nature.com/articles/s41586-021-04301-9) kept from melting or exploding by models we can’t fully understand therefore can’t reliably control. The alternative is scientific pantomime.

---

For all of us, in the decades to come, the real question is: *do we care about understanding?*

Do we care about understanding ourselves, understanding each other, understanding the world around us? Or do we care more about what we can do? If we can unlock a new scope and scale of power over ourselves, over each other, over nature itself, do we care if we understand it? Or will the ends be more important than the means?

Art is a means to understanding ourselves and each other more deeply. But do we care? Or will it be enough to give us the power to speak or type to our devices, and let them produce a simulacrum of this for us? It is no doubt going to be cheaper, faster, and easier.

Science is a means to understanding the world around us more deeply. But do we care? Or will it be enough to collect as much data as we can, feed it to our devices, and let them take care of figuring it out? Many people already seem content to conduct science in this way.

I'm begging us to care more.
